# 6. 성능 개선 & 스트리밍 기반 응답 처리

6-1. GPT 응답 스트리밍 처리
> 🎯 목표
> GPT 응답을 한 줄씩 끊어서 실시간으로 화면에 출력
> LangChain.js의 streaming 기능 + onTokenStream 핸들러 사용

```ts
const chat = new ChatOpenAI({
        modelName: "gpt-4o-mini",
        temperature: 0.7,
        openAIApiKey: process.env.OPENAI_API_KEY,
        streaming: true,
        callbacks:[
            {
                handleLLMNewToken(token:string){
                    onToken(token)
                }
            }
        ]
    });
```

6-2. Debounce 처리
> 목표
> 사용자가 질문을 입력할 때마다 바로 API 요청이 나가지 않게
> 입력이 잠시 멈춘 후에만 요청되도록 debounce 적용
>
> 

6-3. 간단한 캐싱 로직 추가